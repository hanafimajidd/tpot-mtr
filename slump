Code cell <_zS_tvapK-Xt>
# %% [code]
!pip install tpot
from tpot import TPOTRegressor
import pandas as pd

Execution output from May 19, 2024 10:49 PM
4KB
	Stream
		Collecting tpot
		  Downloading TPOT-0.12.2-py3-none-any.whl (87 kB)
		[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m87.4/87.4 kB[0m [31m3.0 MB/s[0m eta [36m0:00:00[0m
		[?25hRequirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.10/dist-packages (from tpot) (1.25.2)
		Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from tpot) (1.11.4)
		Collecting scikit-learn>=1.4.1 (from tpot)
		  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)
		[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m12.1/12.1 MB[0m [31m51.4 MB/s[0m eta [36m0:00:00[0m
		[?25hCollecting deap>=1.2 (from tpot)
		  Downloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)
		[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m135.4/135.4 kB[0m [31m17.3 MB/s[0m eta [36m0:00:00[0m
		[?25hCollecting update-checker>=0.16 (from tpot)
		  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)
		Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.10/dist-packages (from tpot) (4.66.4)
		Collecting stopit>=1.1.1 (from tpot)
		  Downloading stopit-1.1.2.tar.gz (18 kB)
		  Preparing metadata (setup.py) ... [?25l[?25hdone
		Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from tpot) (2.0.3)
		Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from tpot) (1.4.2)
		Requirement already satisfied: xgboost>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tpot) (2.0.3)
		Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->tpot) (2.8.2)
		Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->tpot) (2023.4)
		Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.2->tpot) (2024.1)
		Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.1->tpot) (3.5.0)
		Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from update-checker>=0.16->tpot) (2.31.0)
		Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.24.2->tpot) (1.16.0)
		Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.3.2)
		Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.7)
		Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.0.7)
		Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2024.2.2)
		Building wheels for collected packages: stopit
		  Building wheel for stopit (setup.py) ... [?25l[?25hdone
		  Created wheel for stopit: filename=stopit-1.1.2-py3-none-any.whl size=11938 sha256=53aab80c33421a975a5387493ab7ad57156a54802614d1824bc8449f473dfa26
		  Stored in directory: /root/.cache/pip/wheels/af/f9/87/bf5b3d565c2a007b4dae9d8142dccc85a9f164e517062dd519
		Successfully built stopit
		Installing collected packages: stopit, deap, update-checker, scikit-learn, tpot
		  Attempting uninstall: scikit-learn
		    Found existing installation: scikit-learn 1.2.2
		    Uninstalling scikit-learn-1.2.2:
		      Successfully uninstalled scikit-learn-1.2.2
		Successfully installed deap-1.4.1 scikit-learn-1.4.2 stopit-1.1.2 tpot-0.12.2 update-checker-0.18.0

Code cell <IMZ58d4XLCiP>
# %% [code]
#importing data from a local CSV file
from google.colab import files
uploaded = files.upload()

Execution output from May 19, 2024 10:50 PM
6KB
	Stream
		Saving csv_result-slump.csv to csv_result-slump.csv

Code cell <NS_siyrXLaYS>
# %% [code]
import pandas as pd
df = pd.read_csv("csv_result-slump.csv")
df.head()
y_col = [
'SLUMP_cm',
'FLOW_cm',
'Compressive_Strength_Mpa'

]
y = df[y_col]
#x_col_id = ['id']
#x_id = df[x_col_id]
y.head()
#X_col = ['Xloc','Yloc','Cr','Ni','Zn','Co']
#X = df[X_col]
#X = df[df.columns.drop(y)]
#X=df.drop(columns=['id'])
X=df[df.columns.drop(y)]
#X=df.drop(y,x_id)
y.head()

Execution output from Apr 15, 2024 10:18 PM
10KB
	text/plain
		SLUMP_cm  FLOW_cm  Compressive_Strength_Mpa
		0      23.0     62.0                     34.99
		1       0.0     20.0                     41.14
		2       1.0     20.0                     41.81
		3       3.0     21.5                     42.08
		4      20.0     64.0                     26.82

Code cell <3aG6vRPIYLwK>
# %% [code]
X
Execution output from Feb 1, 2024 8:31 PM
13KB
	text/plain
		Cemment   Slag  Fly_ash  Water    SP  Coarse_Aggr  Fine_Aggr
		0      273.0   82.0    105.0  210.0   9.0        904.0      680.0
		1      163.0  149.0    191.0  180.0  12.0        843.0      746.0
		2      162.0  148.0    191.0  179.0  16.0        840.0      743.0
		3      162.0  148.0    190.0  179.0  19.0        838.0      741.0
		4      154.0  112.0    144.0  220.0  10.0        923.0      658.0
		..       ...    ...      ...    ...   ...          ...        ...
		98     248.3  101.0    239.1  168.9   7.7        954.2      640.6
		99     248.0  101.0    239.9  169.1   7.7        949.9      644.1
		100    258.8   88.0    239.6  175.3   7.6        938.9      646.0
		101    297.1   40.9    239.9  194.0   7.5        908.9      651.8
		102    348.7    0.1    223.1  208.5   9.6        786.2      758.1
		
		[103 rows x 7 columns]

Code cell <xAGKSkW9BPYM>
# %% [code]
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size = 0.20, random_state = 123)
X_train.fillna(X_train.mean(), inplace=True)
X_test.fillna(X_test.mean(), inplace=True)
y_train.fillna(y_train.mean(), inplace=True)
y_test.fillna(y_test.mean(), inplace=True)

from sklearn.preprocessing import MinMaxScaler

from sklearn.compose import TransformedTargetRegressor

# Initialize the scaler
scaler=MinMaxScaler(feature_range=(0,1))

# Choose a transformer for the target variable (e.g., StandardScaler)
target_transformer = scaler


Code cell <lLOiEzntZPyu>
# %% [code]
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
from sklearn.svm import LinearSVR
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
from sklearn.multioutput import MultiOutputRegressor

rfg = MultiOutputRegressor(TPOTRegressor(generations=5, population_size=50,verbosity=2, random_state=123,n_jobs=1, max_time_mins=None, max_eval_time_mins=5, cv=10,scoring='neg_mean_squared_error'))
# Create a TransformedTargetRegressor
model = TransformedTargetRegressor(regressor=rfg, transformer=target_transformer)
model.fit(X_train, y_train)

#rfg.fit(X_train, y_train)
#y_pred = rfg.predict(X_test)
#rfg.score(X_test, y_test)
y_pred=model.predict(X_test)
model.score(X_test, y_test)


Execution output from Apr 8, 2024 11:42 PM
3KB
	Stream
		Generation 1 - Current best internal CV score: -0.0644975311778812
		
		Generation 2 - Current best internal CV score: -0.06378922267931406
		
		Generation 3 - Current best internal CV score: -0.06378922267931406
		
		Generation 4 - Current best internal CV score: -0.06332060943824977
		
		Generation 5 - Current best internal CV score: -0.06257991501744636
		
		Best pipeline: RandomForestRegressor(StandardScaler(input_matrix), bootstrap=False, max_features=0.05, min_samples_leaf=3, min_samples_split=2, n_estimators=100)
		Generation 1 - Current best internal CV score: -0.05149801364582098
		
		Generation 2 - Current best internal CV score: -0.05030929037771931
		
		Generation 3 - Current best internal CV score: -0.05017634103210579
		
		Generation 4 - Current best internal CV score: -0.05005395236449663
		
		Generation 5 - Current best internal CV score: -0.04708408662044418
		
		Best pipeline: ExtraTreesRegressor(ZeroCount(RidgeCV(input_matrix)), bootstrap=True, max_features=0.55, min_samples_leaf=1, min_samples_split=6, n_estimators=100)
		Generation 1 - Current best internal CV score: -0.0045030665796360345
		
		Generation 2 - Current best internal CV score: -0.0045030665796360345
		
		Generation 3 - Current best internal CV score: -0.003721065408579275
		
		Generation 4 - Current best internal CV score: -0.00045847556033925044
		
		Generation 5 - Current best internal CV score: -0.00045847556033925044
		
		Best pipeline: RidgeCV(PolynomialFeatures(input_matrix, degree=2, include_bias=False, interaction_only=False))
		/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but RidgeCV was fitted with feature names
		  warnings.warn(
		/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but RidgeCV was fitted with feature names
		  warnings.warn(
	text/plain
		Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]
		Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]
		Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]
		0.6179480924711758

Code cell <cs8E2TU4EiT3>
# %% [code]
import numpy as np
from statistics import mean
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
rmse1=np.sqrt(mean_squared_error(y_test, y_pred))
print('RMSE  is',rmse1)
mae1=mean_absolute_error(y_test, y_pred)
print('MAE is',mae1)

# Calculate RMSE for each output
rmse_per_output = np.sqrt(mean_squared_error(y_test, y_pred, multioutput='raw_values'))
print('rmse_per_output:',rmse_per_output)


def relative_root_mean_squared_error(true, pred):
    n = len(true) # update
    #rmse_per_output = np.sqrt(mean_squared_error(y_test, y_pred, multioutput='raw_values'))
    num = np.sum((np.square(true - pred)) / n)  # update
    den = np.sum(np.square(pred))
    squared_error = num/den
    rrmse_loss = np.sqrt(squared_error)
    return rrmse_loss

rrmse2=relative_root_mean_squared_error(y_test,y_pred)
arrmse2=mean(rrmse2)
percentage_average_relative_rmse2 = arrmse2 * 100
print("rrmse =",rrmse2)
print("arrmse =", arrmse2)
print(f"Percentage arrmse = {percentage_average_relative_rmse2:.2f}%")
Execution output from Apr 8, 2024 11:44 PM
0KB
	Stream
		RMSE  is 7.706669791419308
		MAE is 4.534979784803904
		rmse_per_output: [ 5.666064   12.06862671  0.64980462]
		rrmse = SLUMP_cm                    0.018850
		FLOW_cm                     0.040150
		Compressive_Strength_Mpa    0.002162
		dtype: float64
		arrmse = 0.020387471149574822
		Percentage arrmse = 2.04%

Code cell <K2nLubLYFz3e>
# %% [code]
from tables import Column
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.pyplot as plt
# taking all rows but only 6 columns
df_small=pd.DataFrame(y_pred)
#df_small = y_test.iloc[:,:3]
features=['SLUMP_cm',
'FLOW_cm',
'Compressive_Strength_Mpa']
correlation_mat = df_small.corr('pearson')
sns.heatmap(correlation_mat, annot = True,yticklabels=features,xticklabels=features)
plt.show()
Execution output from Apr 8, 2024 11:44 PM
30KB
	text/plain
		<Figure size 640x480 with 2 Axes>

Code cell <-H3zV8aAKR4V>
# %% [code]
import pandas as pd
df = pd.read_csv("csv_result-slump.csv")
df.head()
y_col = [
'SLUMP_cm',
'FLOW_cm',
'Compressive_Strength_Mpa'

]
y = df[y_col]
#x_col_id = ['id']
#x_id = df[x_col_id]
y.head()
#X_col = ['Xloc','Yloc','Cr','Ni','Zn','Co']
#X = df[X_col]
#X = df[df.columns.drop(y)]
#X=df.drop(columns=['id'])
X=df[df.columns.drop(y)]
#X=df.drop(y,x_id)
y.head()
Execution output from Apr 8, 2024 10:24 PM
10KB
	text/plain
		SLUMP_cm  FLOW_cm  Compressive_Strength_Mpa
		0      23.0     62.0                     34.99
		1       0.0     20.0                     41.14
		2       1.0     20.0                     41.81
		3       3.0     21.5                     42.08
		4      20.0     64.0                     26.82

Code cell <B5i8MTvPQZMf>
# %% [code]
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size = 0.20, random_state = 123)
X_train.fillna(X_train.mean(), inplace=True)
X_test.fillna(X_test.mean(), inplace=True)
y_train.fillna(y_train.mean(), inplace=True)
y_test.fillna(y_test.mean(), inplace=True)
X_train.head(10)
Execution output from Apr 7, 2024 12:33 AM
14KB
	text/plain
		Cemment   Slag  Fly_ash  Water   SP  Coarse_Aggr  Fine_Aggr
		50    320.0    0.0    163.0  188.0  9.0        866.0      776.0
		29    314.0    0.0    161.0  207.0  6.0        851.0      757.0
		54    322.0    0.0    149.0  186.0  8.0        951.0      709.0
		19    296.0   97.0      0.0  219.0  9.0        932.0      685.0
		59    140.0  128.0    164.0  237.0  6.0        869.0      656.0
		33    274.0   89.0    115.0  202.0  9.0        759.0      827.0
		96    215.6  112.9    239.0  198.7  7.4        884.0      649.1
		82    160.2    0.3    240.0  233.5  9.2        781.0      841.1
		9     304.0    0.0    140.0  214.0  6.0        895.0      722.0
		79    141.1    0.6    209.5  188.8  4.6        996.1      789.2

Code cell <nH8fAIQ-WTCn>
# %% [code]
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size = 0.20, random_state = 123)
X_train.fillna(X_train.mean(), inplace=True)
X_test.fillna(X_test.mean(), inplace=True)
y_train.fillna(y_train.mean(), inplace=True)
y_test.fillna(y_test.mean(), inplace=True)

from sklearn.preprocessing import MinMaxScaler,StandardScaler

from sklearn.compose import TransformedTargetRegressor

# Initialize the scaler
scaler=MinMaxScaler(feature_range=(0,1))
#scaler=StandardScaler()

# Choose a transformer for the target variable (e.g., StandardScaler)
target_transformer = scaler

Code cell <BZc_z-qFGwNa>
# %% [code]
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
from sklearn.svm import LinearSVR
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.multioutput import RegressorChain

reg=LinearSVR(C=0.1, dual=False, fit_intercept=True, max_iter=1000,random_state=123,loss='squared_epsilon_insensitive')
chain = RegressorChain(base_estimator=reg, order=[0,1,2])
# Create a TransformedTargetRegressor
from sklearn.compose import TransformedTargetRegressor
model = TransformedTargetRegressor(regressor=chain, transformer=target_transformer)
model.fit(X_train, y_train)
y_pred2 = model.predict(X_test)
model.score(X_test, y_test)
Execution output from Apr 8, 2024 10:28 PM
0KB
	text/plain
		0.5311519641623185

Code cell <YvgsgfLnZtnb>
# %% [code]
import math
import numpy as np
from statistics import mean
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
rmse2=math.sqrt(mean_squared_error(y_test, y_pred2))
print('RMSE  is',rmse2)
mae2=mean_absolute_error(y_test, y_pred2)
print('MAE is',mae2)




# Calculate RMSE for each output
rmse_per_output = np.sqrt(mean_squared_error(y_test, y_pred2, multioutput='raw_values'))
print('rmse_per_output:',rmse_per_output)


def relative_root_mean_squared_error(true, pred):
    n = len(true) # update
    num = np.sum(np.square(true - pred)) / n  # update
    den = np.sum(np.square(pred))
    squared_error = num/den
    rrmse_loss = np.sqrt(squared_error)
    return rrmse_loss


rrmse2=relative_root_mean_squared_error(y_test,y_pred2)
arrmse2=np.mean(rrmse2)
percentage_average_relative_rmse2 = arrmse2 * 100
print("rrmse =",rrmse2)
print("arrmse =", arrmse2)
print(f"Percentage arrmse = {percentage_average_relative_rmse2:.2f}%")
Execution output from Apr 8, 2024 10:28 PM
0KB
	Stream
		RMSE  is 7.582512363027097
		MAE is 5.339718424972094
		rmse_per_output: [ 5.9173182  11.22178367  3.39711608]
		rrmse = SLUMP_cm                    0.018553
		FLOW_cm                     0.035184
		Compressive_Strength_Mpa    0.010651
		dtype: float64
		arrmse = 0.021462694593671196
		Percentage arrmse = 2.15%

Code cell <OTjOEZpaJWHU>
# %% [code]
from tables import Column
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.pyplot as plt
# taking all rows but only 6 columns
df_small=pd.DataFrame(y_pred2)
#df_small = y_test.iloc[:,:3]
features=['SLUMP_cm',
'FLOW_cm',
'Compressive_Strength_Mpa']
correlation_mat = df_small.corr('pearson')
sns.heatmap(correlation_mat, annot = True,yticklabels=features,xticklabels=features)
plt.show()
Execution output from Apr 8, 2024 10:28 PM
30KB
	text/plain
		<Figure size 640x480 with 2 Axes>

Code cell <buNFEDI9KVYw>
# %% [code]
import pandas as pd
df = pd.read_csv("csv_result-slump.csv")
df.head()
y_col = [
'SLUMP_cm',
'FLOW_cm',
'Compressive_Strength_Mpa'

]
y = df[y_col]
#x_col_id = ['id']
#x_id = df[x_col_id]
y.head()
#X_col = ['Xloc','Yloc','Cr','Ni','Zn','Co']
#X = df[X_col]
#X = df[df.columns.drop(y)]
#X=df.drop(columns=['id'])
X=df[df.columns.drop(y)]
#X=df.drop(y,x_id)
y.head()
Execution output from May 19, 2024 11:12 PM
10KB
	text/plain
		SLUMP_cm  FLOW_cm  Compressive_Strength_Mpa
		0      23.0     62.0                     34.99
		1       0.0     20.0                     41.14
		2       1.0     20.0                     41.81
		3       3.0     21.5                     42.08
		4      20.0     64.0                     26.82

Code cell <DlmIZ41rKaga>
# %% [code]
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size = 0.20, random_state = 123)
X_train.fillna(X_train.mean(), inplace=True)
X_test.fillna(X_test.mean(), inplace=True)
y_train.fillna(y_train.mean(), inplace=True)
y_test.fillna(y_test.mean(), inplace=True)
X_train.head(10)
Execution output from May 15, 2024 12:41 AM
14KB
	text/plain
		Cemment   Slag  Fly_ash  Water   SP  Coarse_Aggr  Fine_Aggr
		50    320.0    0.0    163.0  188.0  9.0        866.0      776.0
		29    314.0    0.0    161.0  207.0  6.0        851.0      757.0
		54    322.0    0.0    149.0  186.0  8.0        951.0      709.0
		19    296.0   97.0      0.0  219.0  9.0        932.0      685.0
		59    140.0  128.0    164.0  237.0  6.0        869.0      656.0
		33    274.0   89.0    115.0  202.0  9.0        759.0      827.0
		96    215.6  112.9    239.0  198.7  7.4        884.0      649.1
		82    160.2    0.3    240.0  233.5  9.2        781.0      841.1
		9     304.0    0.0    140.0  214.0  6.0        895.0      722.0
		79    141.1    0.6    209.5  188.8  4.6        996.1      789.2

Code cell <0i9rP3xCyUQ8>
# %% [code]
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size = 0.20, random_state = 123)
X_train.fillna(X_train.mean(), inplace=True)
X_test.fillna(X_test.mean(), inplace=True)
y_train.fillna(y_train.mean(), inplace=True)
y_test.fillna(y_test.mean(), inplace=True)
from sklearn.preprocessing import MinMaxScaler,StandardScaler

from sklearn.compose import TransformedTargetRegressor

# Initialize the scaler
scaler=MinMaxScaler(feature_range=(0,1))

#scaler=StandardScaler()
X_train=scaler.fit_transform(X_train)
X_test=scaler.fit_transform(X_test)
y_train=scaler.fit_transform(y_train)
y_test=scaler.fit_transform(y_test)

Code cell <alnp0Cm1MPEx>
# %% [code]
from sklearn.svm import LinearSVR
class MultiOutputTP(object):
  def __init__(self, *args, **kwargs):
    self.args = args
    self.kwargs = kwargs
  def fit(self, X, y):
    X, y = map(np.atleast_2d, (X, y))
    assert X.shape[0] == y.shape[0]
    yy = y.shape[1]
    self.regs = []
    for i in range(yy):
#while i <= Ny:
      reg = TPOTRegressor(generations=5, population_size=50,verbosity=2, random_state=123,n_jobs=1, max_time_mins=None, max_eval_time_mins=5, cv=10,scoring='neg_mean_squared_error')
      #reg = LinearSVR(epsilon=0.0, tol=0.0001, C=1.0, loss='epsilon_insensitive', fit_intercept=True, intercept_scaling=1.0, dual=True, verbose=1, random_state=123, max_iter=10000)
      Xi = np.hstack([X, y[:, :i]])
      yi = y[:, i]
      self.regs.append(reg.fit(Xi, yi))


    return self
  def predict(self, X):
    y = np.empty([X.shape[0], len(self.regs)])
    for i, reg in enumerate(self.regs):
      y[:, i] = reg.predict(np.hstack([X, y[:, :i]]))
    return y


Code cell <YnbYahFmNMGN>
# %% [code]
import numpy as np
from tpot import decorators
#decorators.MAX_EVAL_SECS = 240
reg2 = MultiOutputTP(1).fit(X_train, y_train)
#X = np.dtype('float64')



Execution output from May 19, 2024 11:28 PM
3KB
	Stream
		Generation 1 - Current best internal CV score: -0.06612688822897088
		
		Generation 2 - Current best internal CV score: -0.06612688822897088
		
		Generation 3 - Current best internal CV score: -0.06393637157819856
		
		Generation 4 - Current best internal CV score: -0.0630703195557206
		
		Generation 5 - Current best internal CV score: -0.06215682773829338
		
		Best pipeline: RandomForestRegressor(ZeroCount(input_matrix), bootstrap=True, max_features=0.8, min_samples_leaf=3, min_samples_split=2, n_estimators=100)
		Generation 1 - Current best internal CV score: -0.00924718000006248
		
		Generation 2 - Current best internal CV score: -0.009194507933222032
		
		Generation 3 - Current best internal CV score: -0.009114919021015893
		
		Generation 4 - Current best internal CV score: -0.008718592282351457
		
		Generation 5 - Current best internal CV score: -0.00866110837423679
		
		Best pipeline: ElasticNetCV(MaxAbsScaler(PolynomialFeatures(input_matrix, degree=2, include_bias=False, interaction_only=False)), l1_ratio=0.1, tol=0.001)
		Generation 1 - Current best internal CV score: -0.0044184410975068025
		
		Generation 2 - Current best internal CV score: -0.004305151503661399
		
		Generation 3 - Current best internal CV score: -0.0004939972353996076
		
		Generation 4 - Current best internal CV score: -0.0004939972353996076
		
		Generation 5 - Current best internal CV score: -0.0004939972353996076
		
		Best pipeline: ElasticNetCV(PolynomialFeatures(input_matrix, degree=2, include_bias=False, interaction_only=False), l1_ratio=0.8500000000000001, tol=1e-05)
	text/plain
		Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]
		Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]
		Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]

Code cell <9qoxI-A0NNkI>
# %% [code]
Ypred2 = reg2.predict(X_test)

Code cell <4FkjwrhqhUk1>
# %% [code]
from sklearn.metrics import r2_score
r2 = np.abs(r2_score(y_test, Ypred2))
print('r2 score for perfect model is', r2)
Execution output from May 19, 2024 11:45 PM
0KB
	Stream
		r2 score for perfect model is 0.3580872230039347

Code cell <LT7WgdDggzkn>
# %% [code]
y_test=scaler.inverse_transform(y_test)
Ypred2=scaler.inverse_transform(Ypred2)

Code cell <r_UmJIMdJcmA>
# %% [code]
from tables import Column
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.pyplot as plt
# taking all rows but only 6 columns
df_small=pd.DataFrame(Ypred2)
#df_small = y_test.iloc[:,:3]
features=['SLUMP_cm',
'FLOW_cm',
'Compressive_Strength_Mpa']
correlation_mat = df_small.corr('pearson')
sns.heatmap(correlation_mat, annot = True,yticklabels=features,xticklabels=features)
plt.show()
Execution output from May 19, 2024 11:45 PM
32KB
	text/plain
		<Figure size 640x480 with 2 Axes>

Code cell <LOOQTBmHXbOF>
# %% [code]
import math
import numpy as np
from statistics import mean
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
rmse3=math.sqrt(mean_squared_error(y_test, Ypred2))
print('RMSE  is',rmse3)
mae3=mean_absolute_error(y_test, Ypred2)
print('MAE  is',mae3)




# Calculate RMSE for each output
rmse_per_output = np.sqrt(mean_squared_error(y_test, Ypred2, multioutput='raw_values'))
print('rmse_per_output:',rmse_per_output)

def relative_root_mean_squared_error(true, pred):
    n = len(true) # update
    num = np.sum(np.square(true - pred)) / n  # update
    den = np.sum(np.square(pred))
    squared_error = num/den
    rrmse_loss = np.sqrt(squared_error)
    return rrmse_loss


rrmse3=relative_root_mean_squared_error(y_test,Ypred2)
arrmse3=np.mean(rrmse3)
percentage_average_relative_rmse3 = arrmse3 * 100
print("relative_root_mean_squared_error =", rrmse3)
print("arrmse =", arrmse3)
print(f"Percentage arrmse = {percentage_average_relative_rmse3:.2f}%")
Execution output from May 19, 2024 11:45 PM
0KB
	Stream
		RMSE  is 8.1879527796066
		MAE  is 6.349923507845767
		rmse_per_output: [ 5.48083519 11.67006439  5.90743218]
		relative_root_mean_squared_error = 0.04999902261027775
		arrmse = 0.04999902261027775
		Percentage arrmse = 5.00%


